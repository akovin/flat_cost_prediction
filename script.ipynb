{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Автоматизация определения цен на недвижимость в городе Москва\n",
    "__Цель работы__: Цель работы: разработка автоматизированной программной системы для определения цен на недвижимость в г. Москва по известным характеристикам: округ, ближайшая станция метро, расстояние от центра Москвы, время в пути до станции метро пешком, общая площадь квартиры, количество комнат.  \n",
    "</br>\n",
    "![картинка](./moscow_city.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обоснование выбора методов предобработки данных и оценки качества регрессии:\n",
    "* Выполнено исследование данных, значения поля price обрезаны по max и min значениям, подобранным эмпирически. Таким образом, из базы данных были убраны выбросы и аномальные значения, которые могут повлиять на качество работы программы. Также, это позволило задать ценовой диапазон для рассматриваемой недвижимости, повысить адаптацию модели к конкретным данным.\n",
    "\n",
    "* Категорийные признаки переведены в бинарные с помощью класса OneHotEncoder. OneHotEncoder преобразует категорийные данные в числовой формат, что необходимо для большинства алгоритмов машинного обучения. Это позволяет модели корректно обрабатывать категорийные признаки и улучшает её точность.\n",
    "\n",
    "* Получены несколько числовых признаков на основе уже имеющихся, с применением класса NumericPower. Создание новых числовых признаков на основе существующих может выявить скрытые зависимости и улучшить качество модели. Класс NumericPower позволяет легко генерировать такие признаки, что способствует более глубокому анализу данных.\n",
    "\n",
    "* Числовые признаки стандартизованы с использованием функции StandardScaler. Стандартизация приводит все числовые признаки к одному масштабу, это улучшает сходимость алгоритмов и повышает точность работы модели.\n",
    "\n",
    "* Для оценки качества работы программы, использованы метрики: средняя суммарная ошибка, средняя суммарная процентная ошибка. Средняя суммарная ошибка показывает абсолютное отклонение предсказанных значений от фактических, а средняя суммарная процентная ошибка — относительное отклонение. Эти метрики помогают оценить точность работы модели в понятном для человека виде.\n",
    "\n",
    "### Обоснование выбора методов обучения нейронной сети:\n",
    "* Регуляризация с коэффициентом 0.5: Регуляризация помогает предотвратить переобучение модели, добавляя штраф за сложность модели. Коэффициент 0.5 выбран для достижения баланса между переобучением и недообучением, что позволяет модели лучше обобщать данные.\n",
    "\n",
    "* Функция активации ReLU: ReLU (Rectified Linear Unit) является одной из самых часто применяемых функций активации благодаря своей простоте и эффективности. Она помогает избежать проблемы затухания градиента, что ускоряет обучение и улучшает качество работы глубоких нейронных сетей.\n",
    "\n",
    "* Оптимизатор Adam со скоростью обучения 0.001: Adam (Adaptive Moment Estimation) сочетает преимущества двух других методов оптимизации: AdaGrad и RMSProp. Он адаптирует скорость обучения для каждого параметра, что делает его более эффективным и устойчивым к шуму. Скорость обучения 0.001 является стандартным значением, которое часто используется для достижения хороших результатов.\n",
    "\n",
    "* Среднеквадратичная функция ошибки (MSE): MSE (Mean Squared Error) широко используется для задач регрессии, так как она измеряет среднее квадратичное отклонение предсказанных значений от истинных. Это позволяет модели минимизировать большие ошибки и улучшить точность предсказаний.\n",
    "\n",
    "* Число эпох обучения 500: Количество эпох определяет, сколько раз модель будет проходить через весь тренировочный набор данных. Число 500 было подобрано таким образом, чтобы модель хорошо обучилась, но при этом все последующие эпохи не улучшали значительно качество предсказаний."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обоснование выбора алгоритма\n",
    "\n",
    "При решении задачи регрессии были применены 4 алгоритма: регрессор CatBoostRegressor, случайный лес (Scikit Learn), k-случайных соседей (Scikit Learn), нейронная сеть (Keras).\n",
    "\n",
    "Лучшие результаты показал алгоритм CatBoostRegressor, показав значение метрики средней ошибки в процентах 12.9%.\n",
    "\n",
    "__CatBoostRegressor__ CatBoostRegressor работает на основе алгоритма градиентного бустинга, который строит ансамбль деревьев решений. Основные принципы его работы:\n",
    "\n",
    "1. __Градиентый бустинг__: Алгоритм строит модель поэтапно, добавляя новые деревья решений, которые исправляют ошибки предыдущих деревьев. На каждом этапе строится новое дерево, которое минимизирует ошибку предсказания текущей модели.\n",
    "2. __Обработка категориальных признаков__: CatBoost предназначен для эффективной обработки данных, содержащих категориальные признаки.\n",
    "3. __Регуляризация__: Алгоритм включает механизмы регуляризации, которые помогают избежать переобучения и улучшить обобщающую способность модели.\n",
    "4. __Случайность__: CatBoost использует случайные перестановки данных и случайные подвыборки признаков, что помогает улучшить устойчивость модели и уменьшить вероятность переобучения.\n",
    "5. __Обработка пропущенных значений__: Алгоритм умеет эффективно работать с пропущенными значениями в данных, что делает его более гибким и удобным в использовании."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Описание набора данных\n",
    "Для обучения и тестирования используется набор данных, состоящий из 3605 строк.  \n",
    "Набор для обучения состоит из 2884 строк.  \n",
    "Набор для тестирования состоит из 721 строк.  \n",
    "\n",
    "7 колонок:\n",
    "- Округ\n",
    "- Метро\n",
    "- Расстояние от центра (км.)\n",
    "- Время до станции метро (мин.)\n",
    "- Общая площадь (м2)\n",
    "- Число комнат\n",
    "- Стоимость (р.)  \n",
    "\n",
    "Округ: 10 названий округов Москвы\n",
    "* ВАО\n",
    "* ЗАО\n",
    "* Новая Москва\n",
    "* САО\n",
    "* СВАО\n",
    "* СЗАО\n",
    "* ЦАО\n",
    "* ЮАО\n",
    "* ЮВАО\n",
    "* ЮЗАО\n",
    "\n",
    "Метро: 245 названий станций метро, от Авиамоторная до Яхромская   \n",
    "\n",
    "Расстояние от центра (км.):  \n",
    "min = 2  \n",
    "mean = 12.47  \n",
    "max = 29.7  \n",
    "\n",
    "Время до станции метро (мин.):  \n",
    "min = 1  \n",
    "mean = 17.6  \n",
    "max = 78  \n",
    "\n",
    "Общая площадь (м2):  \n",
    "min = 1.6  \n",
    "mean = 61.7  \n",
    "max = 314  \n",
    "\n",
    "Число комнат:  \n",
    "min = 0  \n",
    "mean = 2.42  \n",
    "max = 6  \n",
    "\n",
    "Стоимость (р.):  \n",
    "min = 6002000  \n",
    "mean = 24216880  \n",
    "max = 99999900  \n",
    "\n",
    "Набор данных собран с сайта по продаже недвижимости \"Квадратный метр\" (m2.ru).  \n",
    "Данные актуальны на 2022 год."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка программы к работе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6875,
     "status": "ok",
     "timestamp": 1726316859259,
     "user": {
      "displayName": "Anton Kovin",
      "userId": "03416663137650504421"
     },
     "user_tz": -180
    },
    "id": "ORidT_FMPvB2",
    "outputId": "85ce03fc-1d26-428e-d893-0a73d8603034"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Импортируем модуль для работы с Google Drive\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Монтируем Google Drive к файловой системе Colab, чтобы иметь доступ к файлам на Google Drive\u001b[39;00m\n\u001b[1;32m      4\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "# Импортируем модуль для работы с Google Drive\n",
    "from google.colab import drive\n",
    "# Монтируем Google Drive к файловой системе Colab, чтобы иметь доступ к файлам на Google Drive\n",
    "drive.mount('/content/drive')\n",
    "# Изменяем текущую рабочую директорию на папку Лаба1 в Google Drive\n",
    "%cd /content/drive/MyDrive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4557,
     "status": "ok",
     "timestamp": 1726316863815,
     "user": {
      "displayName": "Anton Kovin",
      "userId": "03416663137650504421"
     },
     "user_tz": -180
    },
    "id": "zuFLJUSiHCcf",
    "outputId": "cac47b57-ea61-4707-8da6-99f08a29db26"
   },
   "outputs": [],
   "source": [
    "# Устанавливаем библиотеку CatBoost, которая используется для градиентного бустинга на основе деревьев решений\n",
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1726316863815,
     "user": {
      "displayName": "Anton Kovin",
      "userId": "03416663137650504421"
     },
     "user_tz": -180
    },
    "id": "ncjjEtekHCch"
   },
   "outputs": [],
   "source": [
    "# Импортируем библиотеку pandas для работы с данными\n",
    "import pandas as pd\n",
    "# Импортируем библиотеку numpy для работы с массивами и математическими функциями\n",
    "import numpy as np\n",
    "# Импортируем базовые классы для создания собственных трансформеров\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# Импортируем класс StandardScaler для стандартизации данных\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Импортируем классы для создания конвейеров и объединения признаков\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "# Импортируем функции для разделения данных\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Импортируем метрики для оценки качества модели\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "# Импортируем регрессор CatBoost для построения модели\n",
    "from catboost import CatBoostRegressor\n",
    "# Импортируем регрессор на основе случайного леса\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Импортируем регрессор на основе метода ближайших соседей\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "# Импортируем библиотеку joblib для сохранения и загрузки моделей\n",
    "import joblib\n",
    "# Импортируем модуль keras из библиотеки TensorFlow\n",
    "from tensorflow import keras\n",
    "# Импортируем класс Sequential для создания конвейеров\n",
    "from tensorflow.keras.models import Sequential\n",
    "# Импортируем классы Input, Dense и Dropout для создания слоев нейронной сети\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "# Импортируем оптимизатор Adam из библиотеки Keras\n",
    "from keras.optimizers import Adam\n",
    "# Импортируем библиотеку matplotlib для построения графиков\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFri_Rf4HCch"
   },
   "source": [
    "### Определяем классы и методы для работы с данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1726316863815,
     "user": {
      "displayName": "Anton Kovin",
      "userId": "03416663137650504421"
     },
     "user_tz": -180
    },
    "id": "aG2CN2XzHCci"
   },
   "outputs": [],
   "source": [
    "# Класс для выбора определённого столбца из данных\n",
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    # Инициализируем класс с указанием столбца\n",
    "    def __init__(self, column):\n",
    "        # Сохраняем название столбца\n",
    "        self.column = column\n",
    "    \n",
    "    # Метод fit, который ничего не делает, но необходим для совместимости\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    # Метод transform, который возвращает указанный столбец\n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.column]\n",
    "\n",
    "# Класс для выбора одного столбца из данных для дальнейших преобразований\n",
    "class NumberSelector(BaseEstimator, TransformerMixin):\n",
    "    # Инициализируем класс с указанием ключа столбца\n",
    "    def __init__(self, key):\n",
    "        # Сохраняем ключ столбца\n",
    "        self.key = key\n",
    "    \n",
    "    # Метод fit, который ничего не делает, но необходим для совместимости\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    # Метод transform, который возвращает указанный столбец\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]\n",
    "\n",
    "# Класс для кодирования категориальных признаков с помощью метода One-Hot Encoding\n",
    "class OHEEncoder(BaseEstimator, TransformerMixin):\n",
    "    # Инициализируем класс с указанием ключа столбца\n",
    "    def __init__(self, key):\n",
    "        # Сохраняем ключ столбца\n",
    "        self.key = key\n",
    "        # Инициализируем список для хранения новых столбцов\n",
    "        self.columns = []\n",
    "\n",
    "    # Метод fit, который создает список новых столбцов после кодирования\n",
    "    def fit(self, X, y=None):\n",
    "        # Создаем список новых столбцов после кодирования\n",
    "        self.columns = [col for col in pd.get_dummies(X, prefix=self.key).columns]\n",
    "        return self\n",
    "    \n",
    "    # Метод transform, который выполняет кодирование и добавляет отсутствующие столбцы\n",
    "    def transform(self, X):\n",
    "        # Выполняем кодирование данных\n",
    "        X = pd.get_dummies(X, prefix=self.key)\n",
    "        # Получаем список текущих столбцов\n",
    "        test_columns = [col for col in X.columns]\n",
    "        # Добавляем отсутствующие столбцы\n",
    "        for col_ in self.columns:\n",
    "            if col_ not in test_columns:\n",
    "                X[col_] = 0\n",
    "        X = X.astype(int)\n",
    "        # Возвращаем закодированные данные с новыми столбцами\n",
    "        return X[self.columns]\n",
    "\n",
    "# Класс для создания новых числовых признаков путём возведения в степень и логарифмирования\n",
    "class NumericPower(BaseEstimator, TransformerMixin):\n",
    "    # Инициализируем класс с указанием ключа столбца и степени\n",
    "    def __init__(self, key, p=2):\n",
    "        # Сохраняем ключ столбца\n",
    "        self.key = key\n",
    "        # Инициализируем список для хранения новых столбцов\n",
    "        self.columns = []\n",
    "        # Сохраняем степень\n",
    "        self.p = p + 1\n",
    "\n",
    "    # Метод fit, который создает список новых столбцов после преобразования\n",
    "    def fit(self, X, y=None):\n",
    "        # Создаем список новых столбцов после преобразования\n",
    "        B = [self.key + str(i) for i in range(1, self.p)]\n",
    "        self.columns = B + ['log']\n",
    "        return self\n",
    "    \n",
    "    # Метод transform, который выполняет преобразования и возвращает новые столбцы\n",
    "    def transform(self, X):\n",
    "        # Преобразуем данные в массив\n",
    "        Xp = X.values.reshape(-1, 1)\n",
    "        # Выполняем возведение в степень\n",
    "        for i in range(2, self.p):\n",
    "            Xp = np.hstack([Xp, (X.values.reshape(-1, 1) ** i).astype(float)])\n",
    "        # Выполняем логарифмирование\n",
    "        Xp = np.hstack([Xp, np.log(X.values.reshape(-1, 1) + 1).astype(float)])\n",
    "        # Создаем DataFrame с новыми столбцами\n",
    "        B = pd.DataFrame(data=Xp, index=X.index, columns=self.columns)\n",
    "        # Возвращаем DataFrame\n",
    "        return B[self.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iD5KNvY3HCcj"
   },
   "source": [
    "### Определяем функции для создания и обучения конвейеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1726316863815,
     "user": {
      "displayName": "Anton Kovin",
      "userId": "03416663137650504421"
     },
     "user_tz": -180
    },
    "id": "XPpB594EHCcj"
   },
   "outputs": [],
   "source": [
    "# Функция для создания конвейера\n",
    "def get_pipeline(estimator):\n",
    "    # Определяем числовые столбцы\n",
    "    num_cols = ['route_minutes', 'total_area', 'rooms']\n",
    "    # Определяем категориальные столбцы\n",
    "    cat_cols = ['metro', 'okrug']\n",
    "    # Создаем список для хранения финальных трансформеров\n",
    "    final_transformers = list()\n",
    "    \n",
    "    # Для каждого категориального столбца\n",
    "    for cat_col in cat_cols:\n",
    "        # Создаем конвейер для категориальных данных\n",
    "        cat_transformer = Pipeline([\n",
    "            ('selector', FeatureSelector(column=cat_col)),  # Выбираем столбец\n",
    "            ('ohe', OHEEncoder(key=cat_col))  # Применяем One-Hot Encoding\n",
    "        ])\n",
    "        # Добавляем трансформер в список\n",
    "        final_transformers.append((cat_col, cat_transformer))\n",
    "    \n",
    "    # Для каждого числового столбца\n",
    "    for num_col in num_cols:\n",
    "        # Создаем конвейер для числовых данных\n",
    "        cont_transformer = Pipeline([\n",
    "            ('selector', NumberSelector(key=num_col)),  # Выбираем столбец\n",
    "            ('power', NumericPower(key=num_col, p=3)),  # Применяем возведение в степень\n",
    "            ('scale', StandardScaler())  # Стандартизируем данные\n",
    "        ])\n",
    "        # Добавляем трансформер в список\n",
    "        final_transformers.append((num_col, cont_transformer))\n",
    "    \n",
    "    # Объединяем все трансформеры\n",
    "    feats = FeatureUnion(final_transformers)\n",
    "    \n",
    "    # Создаем финальный конвейер с регрессором CatBoost\n",
    "    pipeline = Pipeline([\n",
    "        ('features', feats),  # Добавляем объединенные трансформеры\n",
    "        ('regressor', estimator),  # Добавляем регрессор\n",
    "    ])\n",
    "    \n",
    "    # Возвращаем созданный конвейер\n",
    "    return pipeline\n",
    "\n",
    "# Функция для обучения конвейера\n",
    "def fit_pipeline(X_train, y_train, pipeline, save_model=False):\n",
    "    # Обучаем конвейер на тренировочных данных\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    # Если необходимо, сохраняем модель\n",
    "    if save_model:\n",
    "        joblib.dump(pipeline, 'model.pkl')\n",
    "    # Возвращаем обученный конвейер\n",
    "    return pipeline\n",
    "\n",
    "# Функция для обучения конвейера\n",
    "def main_pipeline(regressor, X_train, y_train, X_test, y_test, save_model=False):\n",
    "    # Создаем конвейер\n",
    "    pipe = get_pipeline(regressor)\n",
    "    # Обучаем конвейер на тренировочных данных и сохраняем модель, если необходимо\n",
    "    pipe = fit_pipeline(X_train, y_train, pipe, save_model)\n",
    "    # Делаем предсказания на тестовых данных\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    # Вычисляем среднюю абсолютную ошибку\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    # Вычисляем среднюю абсолютную процентную ошибку\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    return mae, mape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определяем функции для работы нейронной сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для создания конвейера для предобработки данных для нейронной сети\n",
    "def transform_data_init_nn():\n",
    "    # Определяем числовые столбцы\n",
    "    num_cols = ['route_minutes', 'total_area', 'rooms']\n",
    "    # Определяем категориальные столбцы\n",
    "    cat_cols = ['metro', 'okrug']\n",
    "    # Создаем список для хранения финальных трансформеров\n",
    "    final_transformers = list()\n",
    "    \n",
    "    # Для каждого категориального столбца\n",
    "    for cat_col in cat_cols:\n",
    "        # Создаем конвейер для категориальных данных\n",
    "        cat_transformer = Pipeline([\n",
    "            ('selector', FeatureSelector(column=cat_col)),  # Выбираем столбец\n",
    "            ('ohe', OHEEncoder(key=cat_col))  # Применяем One-Hot Encoding\n",
    "        ])\n",
    "        # Добавляем трансформер в список\n",
    "        final_transformers.append((cat_col, cat_transformer))\n",
    "    \n",
    "    # Для каждого числового столбца\n",
    "    for num_col in num_cols:\n",
    "        # Создаем конвейер для числовых данных\n",
    "        cont_transformer = Pipeline([\n",
    "            ('selector', NumberSelector(key=num_col)),  # Выбираем столбец\n",
    "            ('power', NumericPower(key=num_col, p=3)),  # Применяем возведение в степень\n",
    "            ('scale', StandardScaler())  # Стандартизируем данные\n",
    "        ])\n",
    "        # Добавляем трансформер в список\n",
    "        final_transformers.append((num_col, cont_transformer))\n",
    "    \n",
    "    # Объединяем все трансформеры\n",
    "    feats = FeatureUnion(final_transformers)\n",
    "    \n",
    "    # Создаем финальный конвейер для предобработки данных\n",
    "    pipeline = Pipeline([\n",
    "        ('features', feats)  # Добавляем объединенные трансформеры\n",
    "    ])\n",
    "    \n",
    "    # Возвращаем созданный конвейер\n",
    "    return pipeline\n",
    "\n",
    "# Функция для обучения конвейера для предобработки данных для нейронной сети\n",
    "def transform_data_fit_nn(X_train, y_train, pipeline):\n",
    "    # Обучаем конвейер на тренировочных данных\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    # Возвращаем обученный конвейер\n",
    "    return pipeline\n",
    "\n",
    "# Главная функция для создания и обучения конвейера\n",
    "def transform_data_main_nn(X_train, y_train, X_test):\n",
    "    # Инициализация конвейера\n",
    "    pipe_nn = transform_data_init_nn()\n",
    "    # Обучение конвейера\n",
    "    pipe_nn = transform_data_fit_nn(X_train, y_train, pipe_nn)\n",
    "    # Предобработка данных для обучения\n",
    "    X_train_transformed = pipe_nn.named_steps['features'].transform(X_train)\n",
    "    # Предобработка тестовых данных\n",
    "    X_test_transformed = pipe_nn.named_steps['features'].transform(X_test)\n",
    "    # Возвращаем преобразованные данные для обучения и тестовые\n",
    "    return X_train_transformed, X_test_transformed\n",
    "\n",
    "# Функция для создания модели нейронной сети\n",
    "def model_create_nn():\n",
    "    # Определяем модель\n",
    "    model = Sequential()\n",
    "    # Добавляем входной слой с размерностью 257\n",
    "    model.add(Input(shape=(257,)))\n",
    "    # Добавляем полносвязный слой с 128 нейронами и функцией активации ReLU\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    # Добавляем слой Dropout с коэффициентом 0.5 для регуляризации\n",
    "    model.add(Dropout(0.5))\n",
    "    # Добавляем полносвязный слой с 64 нейронами и функцией активации ReLU\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    # Добавляем слой Dropout с коэффициентом 0.5 для регуляризации\n",
    "    model.add(Dropout(0.5))\n",
    "    # Добавляем полносвязный слой с 32 нейронами и функцией активации ReLU\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    # Добавляем выходной слой с 1 нейроном для регрессии\n",
    "    model.add(Dense(1))\n",
    "    # Компиляция модели с оптимизатором Adam и функцией потерь MSE\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    # Возвращаем созданную модель\n",
    "    return model\n",
    "\n",
    "# Функция для обучения модели\n",
    "def model_fit_nn(model, X_train_transformed, y_train):\n",
    "    # Обучение модели на преобразованных данных\n",
    "    # Число эпох обучения равно 500, логирование обучения выключено\n",
    "    model.fit(X_train_transformed, y_train, epochs=500, verbose=0)\n",
    "    # Возвращаем обученную модель\n",
    "    return model\n",
    "\n",
    "# Функция для тестирования и оценки качества работы модели\n",
    "def model_predict_nn(model, X_test_transformed, y_test):\n",
    "    # Выполняем предсказания на тестовых данных\n",
    "    y_pred = model.predict(X_test_transformed, verbose=0)\n",
    "    # Вычисляем среднюю абсолютную ошибку (MAE)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    # Вычисляем среднюю абсолютную процентную ошибку (MAPE)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    # Возвращаем значения MAE и MAPE\n",
    "    return mae, mape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определяем функции для вывода результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для построения диаграммы метрик MAPE\n",
    "def plot_mape():\n",
    "    # Извлечение значений метрики MAPE\n",
    "    values = list(mape_algorithms.values())\n",
    "    # Умножение каждого значения на 100 для перевода в проценты\n",
    "    values = [v * 100 for v in values]\n",
    "    # Создание столбчатой диаграммы\n",
    "    plt.figure(figsize=(6, 6))  # Определение размера изображения\n",
    "    bars = plt.bar(estimators_plot_names, values, color='blue', width=0.3)  # Построение столбчатой диаграммы\n",
    "    plt.xlabel('Algorithms')  # Определение подписи оси X\n",
    "    plt.ylabel('Mean Average Percentage Error (%)')  # Определение подписи оси Y\n",
    "    plt.title('MAPE metric values of different algorithms')  # Определение заголовка диаграммы\n",
    "    plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{int(x)}'))  # Определение форматирования значений оси Y как целых чисел\n",
    "    # Добавление текстовых подписей столбцам диаграммы\n",
    "    for bar, value in zip(bars, values):\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{round(value, 1)}', ha='center', va='bottom')\n",
    "    # Отображение диаграммы\n",
    "    plt.show()\n",
    "\n",
    "# Функция для построения диаграммы метрик MAE\n",
    "def plot_mae():\n",
    "    # Извлечение значений метрики MAE\n",
    "    values = list(mae_algorithms.values())\n",
    "    # Создание столбчатой диаграммы\n",
    "    plt.figure(figsize=(6, 6))  # Установка размера изображения\n",
    "    bars = plt.bar(estimators_plot_names, values, color='green', width=0.3)  # Построение столбчатой диаграммы\n",
    "    plt.xlabel('Algorithms')  # Определение подписи оси X\n",
    "    plt.ylabel('Mean Average Error (руб.)')  # Определение подписи оси Y\n",
    "    plt.title('MAE metric values of different algorithms')  # Определение заголовка диаграммы\n",
    "    # Добавление подписи значения метрики над столбцами\n",
    "    for bar, value in zip(bars, values):\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{int(value)}', ha='center', va='bottom')\n",
    "    # Отображение диаграммы\n",
    "    plt.show()\n",
    "\n",
    "# Функция для вывода результатов\n",
    "def print_results(mape_algorithms, mae_algorithms):\n",
    "    # Вывод заголовка для значений MAPE\n",
    "    print('\\nMAPE values:')\n",
    "    # Цикл по элементам словаря mape_algorithms\n",
    "    for key, value in mape_algorithms.items():\n",
    "        # Вывод названия алгоритма\n",
    "        print(key)\n",
    "        # Вывод значения MAPE, умноженного на 100 и округленного до 1 знака после запятой\n",
    "        print(f'{round(value * 100, 1)}%')\n",
    "    # Вызов функции для построения диаграммы метрик MAPE\n",
    "    plot_mape()\n",
    "    \n",
    "    # Вывод заголовка для значений MAE\n",
    "    print('\\nMAE values:')\n",
    "    # Цикл по элементам словаря mae_algorithms\n",
    "    for key, value in mae_algorithms.items():\n",
    "        # Вывод названия алгоритма\n",
    "        print(key)\n",
    "        # Вывод значения MAE, округленного до целого числа\n",
    "        print(f'{round(value)} руб.')\n",
    "    # Вызов функции для построения диаграммы метрик MAE\n",
    "    plot_mae()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определяем dict с регрессионными моделями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем словарь с различными моделями регрессии\n",
    "estimators_sklearn = {\n",
    "    # Добавляем модель CatBoostRegressor с заданными параметрами\n",
    "    'Cat Boost Regressor': CatBoostRegressor(iterations=1000, max_depth=10, random_state=42, silent=True),\n",
    "    # Добавляем модель регрессора на основе случайного леса с 100 деревьями\n",
    "    'Random Forest Regressor': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    # Добавляем модель регрессора на основе метода ближайших соседей\n",
    "    'K-Neighbors Regressor': KNeighborsRegressor(n_neighbors=5)\n",
    "}\n",
    "\n",
    "# Список названий алгоритмов для отображения на диаграмме\n",
    "estimators_plot_names = ['Cat Boost', 'RF', 'KNN', 'Neural Network']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pse6rNGRHCcj"
   },
   "source": [
    "### Основной код программы\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 629
    },
    "executionInfo": {
     "elapsed": 772,
     "status": "error",
     "timestamp": 1726316933956,
     "user": {
      "displayName": "Anton Kovin",
      "userId": "03416663137650504421"
     },
     "user_tz": -180
    },
    "id": "9SJim35PHCck",
    "outputId": "eb2fa5c4-2834-4bc0-c78f-132cdcbc1070"
   },
   "outputs": [],
   "source": [
    "# Проверяем, что скрипт выполняется как основная программа\n",
    "if __name__ == '__main__':\n",
    "    # Читаем данные из CSV файла и задаем имена столбцов\n",
    "    df = pd.read_csv(\"./data_moscow.csv\", names=['okrug', 'metro', 'distance_from_center', 'route_minutes', 'total_area', 'rooms', 'price'])\n",
    "    # Выводим размерность датафрейма\n",
    "    print('размерность исходных данных = ', df.shape)\n",
    "    \n",
    "    # Фильтруем данные, оставляя строки с ценой между 6 000 000 и 100 000 000\n",
    "    df = df[(df['price'] > 6_000_000) & (df['price'] < 100_000_000)]\n",
    "    # Выводим размерность отфильтрованного датафрейма\n",
    "    print(\"размерность данных после урезания поля price\\nпо максимальному и минимальному значениям = \", df.shape)\n",
    "    \n",
    "    # Разделяем данные на тренировочный и тестовый наборы\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[['okrug', 'metro', 'distance_from_center', 'route_minutes', 'total_area', 'rooms']], df['price'], test_size=0.2, random_state=42)\n",
    "    # Выводим размерности используемых наборов данных\n",
    "    print('размерность набора для обучения = ', len(y_train))\n",
    "    print('размерность набора для тестирования = ', len(y_test))\n",
    "    # Выводим среднюю стоимость квартиры в тестовом наборе\n",
    "    print('средняя стоимость квартиры в тестовом наборе = ', round(y_test.mean()), 'рубль')\n",
    "    \n",
    "    mae_algorithms = {}\n",
    "    mape_algorithms = {}\n",
    "    \n",
    "    for estimator_name, estimator in estimators_sklearn.items():\n",
    "        if estimator_name in ['Cat Boost Regressor', 'Random Forest Regressor', 'K-Neighbors Regressor']:\n",
    "            mae_algorithms[estimator_name], mape_algorithms[estimator_name] = main_pipeline(estimator, X_train, y_train, X_test, y_test, save_model=False)\n",
    "    \n",
    "    # Выполняем функции, необходимые для работы нейронной сети\n",
    "    # Предобработка тренировочных и тестовых данных с помощью функции transform_data_main_nn\n",
    "    X_train_transformed, X_test_transformed = transform_data_main_nn(X_train, y_train, X_test)\n",
    "    # Создание модели нейронной сети\n",
    "    model_nn = model_create_nn()\n",
    "    # Обучение модели на преобразованных тренировочных данных\n",
    "    model_nn = model_fit_nn(model_nn, X_train_transformed, y_train)\n",
    "    # Предсказание и оценка модели на тестовых данных\n",
    "    mae_nn, mape_nn = model_predict_nn(model_nn, X_test_transformed, y_test)\n",
    "    \n",
    "    # Сохранение значения MAE для нейронной сети в словарь mae_algorithms\n",
    "    mae_algorithms['Neural Network'] = mae_nn\n",
    "    # Сохранение значения MAPE для нейронной сети в словарь mape_algorithms\n",
    "    mape_algorithms['Neural Network'] = mape_nn\n",
    "    \n",
    "    # Вывод результатов\n",
    "    print_results(mape_algorithms, mae_algorithms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e11usnV4HCck"
   },
   "source": [
    "### Выводы\n",
    "* __Исследование данных__: рассмотрены статистические характеристики признаков, определены выбросы данных. Набор данных ограничен по максимальному и минимальному значению поля price.\n",
    "* __Предварительная обработка данных__: категориальные признаки переведены в числовые, применен алгоритм создания дополнительных числовых признаков, данные стандартизованы.\n",
    "* __Выполнено обучение и тестирование__ 4-х алгоритмов: CatBoostRegressor, случайный лес (Scikit Learn), k-случайных соседей (Scikit Learn), нейронная сеть (Keras).\n",
    "* __Выполнена оценка качеcтва работы__ алгоритмов по двум метрикам: Mean Absolute Percentage Error, Mean Absolute Error.\n",
    "Лучшие результаты показал CatBoostRegressor:  \n",
    "MAPE: 12.9%  \n",
    "MAE: 3099514 рублей      \n",
    "* __Построены диаграммы__ со значениями метрик MAPE, MAE \n",
    "    \n",
    "__Данные результаты говорят о том, что разработанную систему нельзя использовать для автоматического назначения цены, но можно использовать в качестве системы для быстрой оценки недвижимости и рекомендации её примерной стоимости.__"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
